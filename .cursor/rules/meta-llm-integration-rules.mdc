---
description: 
globs: 
alwaysApply: true
---
- Use the llama-api-client source for integration help
- Offical docs are here https://llama.developer.meta.com/docs/guides/best-practices/?team_id=1738085153773497 @Web

Tool calling with Llama API
Introduction
You can use tool calling to interface Llama API with external tools that you provide. Tool calling enables you to expand the capabilities of a Llama model in order to gather new information that is not readily available to the model, or to take actions such as sending emails or calling other APIs.
When you enable tool calling via the API, the model automatically decides if it needs one or more of the available tools to respond to your prompt, and if so will tell you which tools it would like to use and how. You will then execute the tool and send back the results as part of a chat completion, so that the model can incorporate the information into its next response.
When to use tool calling
Tool calling can be used for a number of different purposes, which can broadly be grouped as finding information and performing actions.
Finding information
Tool calling enables access to knowledge not included in the model’s training data. Below are some cases where tool calling could help retrieve external information.
•Fetching real-time data: Your request may require access to frequently updated information, such as sports scores or news headlines, weather data when planning a beach trip, or the latest stock values for financial analysis.
•Performing calculations: While language models excel at reasoning, they typically don't perform well with raw arithmetic or complex calculations. Like a human, an AI model can use a basic calculator for math involving large numbers. For specialized calculations such as tax accounting or internal projections, create a custom tool for Llama.
•Search the web: Use a search engine to collect additional information. This provides a general solution for fetching real-time data when you don't want to create a specific tool for each information source.
•Access databases: Access your own private databases to help answer questions about your business or customers. Include up-to-date pricing information or availability.
Performing actions
Beyond finding information, you can use tools to take actions on your behalf. Multi-turn actions enable Llama to become a true AI agent, taking actions and reasoning about results to determine next steps and potentially the next tool.
While the options for tools are endless, here are some common use cases:
•Sending messages: Enable Llama to send emails or chat messages proactively
•Triggering jobs: Start long-running jobs or workflows to run in the background
•Calling other APIs: Make API calls to external services outside of the model
How to use tool calling
Prerequisites
Before you begin, ensure you have:
•A valid Llama API key
•Python 3.7 or higher
Setting up your environment
To use tool calling with Llama API, you'll need to set up authentication using your Llama API key, which you’ll store in an environment variable for now.
Here's how to set up your Python environment for making API calls:
Python
1234567891011
import os
import requests
import json

os.environ["LLAMA_API_KEY"] = "your_api_key_here"

# Load API key from environment variable
LLAMA_API_KEY = os.environ.get('LLAMA_API_KEY')

# Define the base URL

Creating a tool definition
The tool (singular) role is a special role that indicates to the model that the results come from an external tool. When the model uses a tool, it includes a tools (plural) field in the response. After you execute the tool call, return the results to the model using the tool role.
To instruct a Llama model on how to use a tool, create a tool definition that includes:
•The tool’s name
•A description of what the tool does
•The parameters that the tool accepts
The chat completion API accepts a tools parameter, which is an array of tool definitions. This is separate from the messages parameter, which contains the conversation history. It is not necessary to include tool information in the system prompt, or even request that the model use tools at all. If tools are included, the model will by default use them if it decides they are necessary.
Python
123456789101112131415161718192021222324252627282930313233343536
get_weather_tool = {
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Retrieve the current temperature for a specified location",
        "parameters": {
        "properties": {
            "location": {
            "type": "string",
            "description": "The city, state, or country for which to fetch the temperature"

The model will use the provided tool definition to infer whether a tool should be invoked, then respond with a message indicating which tool it wants to use.
JSON response
{
  "completion_message": {
    "content": {
      "type": "text",
      "text": ""
    },
    "role": "assistant",
    "stop_reason": "tool_calls",
    "tool_calls": [
      {

Invoking your tool
Llama API does not have access to any execution environment, and in many cases will not have access to the tool you have defined. Hence your application must execute the tool call generated by the model and return the results.

Return the results to the model using the tool role, as shown below.
Python
123456789101112131415161718192021222324252627282930
payload = {
	"model": "Llama-3.3-8B-Instruct",
	"messages": [
		{"role": "user", "content": "What is the weather in Menlo Park?"},
		{
			"role": "assistant", "content": "", "stop_reason": "tool_calls",
			"tool_calls": [{
                "id": "bb3660bc-7992-4b0c-b1af-04aa424f559c",
                "function": {
                    "name": "get_weather",

Consider using llama-stack-apps or another similar application to execute the tool call and retrieve the results.
Finally, the model uses the results to return an answer to the original user question.
JSON response
{
  "completion_message": {
    "content": {
      "type": "text",
      "text": "The current temperature in Menlo Park is 47\u00b0F."
    },
    "role": "assistant",
    "stop_reason": "stop",
    "tool_calls": []
  },

Testing & debugging
Because tool calls are executed outside of the model, you need to test and debug them separately. However, since there is no requirement to actually call the function, for testing purposes you can return a mock response with the output you expect from your tool. This means you can easily experiment with different tools before they are actually built, allowing you to iterate on your tool definitions and optimize your prompts prior to committing to development.
Sometimes, the model may not call the tool even if it is available. This is because the model may not believe that the tool is the best option for the user's request.

To help the model call the right tool, you can try the following:
•Provide some examples of how the tool should be used in the prompt
•Modify the tool definition to include more specific instructions or examples
•If you know the tool should be used, you can directly ask the model to use the tool in the prompt
Example tools
Tool definitions are entirely up to you, and can be as specific or broad as you would like. Tools often correspond to a specific web API or service, but could also be used to call a library function or perform a calculation. Here are some examples of tools you might use with Llama.
Search the web
JSON
12345678910111213141516
{
  "type": "function",
  "function": {
    "name": "web_search",
    "description": "Search the web for information",
    "parameters": {
      "properties": {
        "query": {
          "type": "string",
          "description": "The query to search for"

Use Wolfram Alpha as a calculator
JSON
1234567891011121314
{
    "type": "function",
    "function": {
        "name": "calculate",
        "description": "Complete mathematical calculations using Wolfram Alpha",
        "properties": {
            "expression": {
                "type": "string",
                "description": "The mathematical expression to evaluate"
            }

Was this page helpful?
Introduction
When to use tool calling
How to use tool calling
Testing & debugging
Example tools
